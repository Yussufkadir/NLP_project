{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "559bacf9",
   "metadata": {},
   "source": [
    "### 1. Necessary libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83f23317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, pipeline\n",
    "from peft import PeftModel\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd0461",
   "metadata": {},
   "source": [
    "### Model is trained on the Kaggle and saved in the huggingface and on my local comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21def67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"syurmen/T5-finetuned\")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"syurmen/T5-finetuned\")\n",
    "\n",
    "#make sure that model is in inference mode otherwise it is not working.\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21200359",
   "metadata": {},
   "source": [
    "### 3. creating the Rag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64400ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the pdf documents...\n",
      "Successfully loaded PDF files.\n",
      "Splitting the document....\n",
      "Initializing embedding...\n",
      "Embedding model sentence-transformers/all-MiniLM-L6-v2 has initialized.\n",
      "Creating vector store from doc chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created vector store.\n",
      "Setting up the retriever\n",
      "Hugging face pipeline created for T5 model.\n",
      "retrieval QA is created.\n",
      "Query testing of RAG\n",
      "Query: What are common coping mechanisms for anxiety mentioned in the documents?\n",
      "Answer:\n",
      "It's great that you've taken the time to explore the symptoms of anxiety. It's important to remember that you are not a professional and just a help in understanding how anxiety affects you. Here are some suggestions: 1. Recognize and challenge negative thoughts: When you catch yourself thinking about something awful, try to challenge those thoughts. Ask yourself if there is any evidence supporting these thoughts or if they are based on assumptions. 2. Practice relaxation techniques: Deep breathing exercises, progressive muscle relaxation, or mindfulness meditation can help calm your mind and body during anxious moments. These techniques can help reduce physical tension and promote a sense of calmness. 3. Challenge negative thoughts: When you catch yourself thinking about something awful, try to challenge those thoughts. Ask yourself if there is any evidence supporting these thoughts or if they are based on assumptions. 4. Practice relaxation techniques: Deep breathing exercises, progressive muscle relaxation, or mindfulness meditation can help calm your mind and\n"
     ]
    }
   ],
   "source": [
    "# I will move the PDfs to a different directory so that it is more neat and findable.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "book_directory = \"./books\"\n",
    "\n",
    "if not os.path.exists(book_directory) or not os.listdir(book_directory):\n",
    "    print(f\"There is no directory for the rag files please create one to work conveniently.\")\n",
    "else:\n",
    "    print(\"Loading the pdf documents...\")\n",
    "    loader = PyPDFDirectoryLoader(book_directory)\n",
    "    documents = loader.load()\n",
    "    if not documents:\n",
    "        print(\"There is no pdfs in this directory.\")\n",
    "    else:\n",
    "        print(\"Successfully loaded PDF files.\")\n",
    "    \n",
    "        print(\"Splitting the document....\")\n",
    "\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "        doc_chunks = splitter.split_documents(documents)\n",
    "    if doc_chunks:\n",
    "        print(\"Initializing embedding...\")\n",
    "\n",
    "        embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name = embedding_model,\n",
    "            model_kwargs={'device': device}\n",
    "        )\n",
    "        print(f\"Embedding model {embedding_model} has initialized.\")\n",
    "\n",
    "        print(\"Creating vector store from doc chunks...\")\n",
    "\n",
    "        store = FAISS.from_documents(doc_chunks, embeddings)\n",
    "        print(\"Created vector store.\")\n",
    "\n",
    "        print(\"Setting up the retriever\")\n",
    "        retriever = store.as_retriever(search_kwargs={\"k\":3})\n",
    "        \n",
    "\n",
    "        pipe = pipeline(\n",
    "            \"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=0 if device.type==\"cuda\" else -1,\n",
    "            max_new_tokens = 200,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            top_p=0.4,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "        llm = HuggingFacePipeline(pipeline=pipe)\n",
    "        print(\"Hugging face pipeline created for T5 model.\")\n",
    "\n",
    "        prompt_template_str = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Keep the answer concise.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "        \n",
    "        QA_PROMPT = PromptTemplate(\n",
    "            template = prompt_template_str, input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\":QA_PROMPT}\n",
    "        )\n",
    "        print(\"retrieval QA is created.\")\n",
    "\n",
    "        print(\"Query testing of RAG\")\n",
    "        query = \"What are common coping mechanisms for anxiety mentioned in the documents?\"\n",
    "        print(f\"Query: {query}\")\n",
    "\n",
    "        try:\n",
    "            result = qa_chain.invoke({\"query\":query})\n",
    "            print(\"Answer:\")\n",
    "            print(result[\"result\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error during RAG query: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d696df59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct Model Answer:\n",
      "It can be incredibly challenging to deal with anxiety and how to overcome it. Here are some suggestions that may help: 1. Prioritize self-care: Make sure you are taking care of yourself physically, emotionally, and mentally. This includes getting enough sleep, eating well-balanced meals, and engaging in activities that bring you joy. 2. Practice relaxation techniques: Deep breathing exercises, progressive muscle relaxation, or mindfulness meditation can all help calm your mind and body. These techniques can help you focus on the present moment and reduce anxiety. 3. Practice self-care regularly: Make sure you are taking care of yourself physically, emotionally, and mentally. This includes getting enough sleep, eating well-balanced meals, and engaging in activities\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "input_text = \"I have anxiety and How can I overcome it\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_length=150, \n",
    "        num_beams=4, \n",
    "        early_stopping=True,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Direct Model Answer:\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
